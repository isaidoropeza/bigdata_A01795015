{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1268ea05-26dc-4d73-8c30-c303f6ff1afd",
      "metadata": {
        "id": "1268ea05-26dc-4d73-8c30-c303f6ff1afd"
      },
      "source": [
        "**MAESTRÍA EN INTELIGENCIA ARTIFICIAL APLICADA**\n",
        "\n",
        "**Curso: TC4034.10 - Análisis de grandes volúmenes de datos**\n",
        "\n",
        "\n",
        "**Tecnológico de Monterrey**\n",
        "\n",
        "**Actividad 4 | Métricas de calidad de resultados**\n",
        "\n",
        "**Alumno**\n",
        "\n",
        "**Isaid Posadas Oropeza A01795015**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91efcb08-7a39-41db-ad80-575971c3000c",
      "metadata": {
        "id": "91efcb08-7a39-41db-ad80-575971c3000c"
      },
      "source": [
        "# 1. Construcción de la muestra M\n",
        "\n",
        "En esta sección se construye la muestra representativa **M** a partir del dataset original recolectado (E-Commerce Behavior Data), aplicando estrategias que garanticen una representación balanceada y sin sesgos de la población **P**.\n",
        "\n",
        "## 1.1 Objetivo\n",
        "\n",
        "Generar una muestra **M** que refleje de manera equitativa los principales comportamientos de los usuarios (`view`, `cart`, `purchase`) y que esté compuesta por subconjuntos **Mi**, definidos a partir de variables de caracterización seleccionadas previamente.\n",
        "\n",
        "## 1.2 Variables de caracterización seleccionadas\n",
        "\n",
        "De acuerdo con el análisis previo en la Actividad 3, las siguientes variables permiten segmentar el comportamiento de los usuarios:\n",
        "\n",
        "- **event_day**: Día de la semana en que ocurrió el evento.\n",
        "- **event_type**: Tipo de evento registrado (view, cart, purchase).\n",
        "- **category_code**: Categoría del producto.\n",
        "- **event_hour**: Hora del evento.\n",
        "- **brand**: Marca del producto.\n",
        "\n",
        "Para efectos de esta actividad, se utilizará como variable principal de particionamiento el **event_day**, generando subconjuntos **Mi**, donde cada **Mi** contiene exclusivamente eventos ocurridos en un mismo día de la semana.\n",
        "\n",
        "## 1.3 Construcción de la muestra balanceada M\n",
        "\n",
        "1. Se realiza un **muestreo estratificado balanceado** por `event_type`, asegurando que `view`, `cart` y `purchase` estén equitativamente representados.\n",
        "2. Se eliminan registros con valores nulos.\n",
        "3. Se eliminan valores atípicos en la variable `price` utilizando los percentiles 1% y 99%.\n",
        "4. Se convierten variables de tipo fecha (`event_time`) en variables derivadas (`event_date`, `event_hour`, `event_day`).\n",
        "5. Se particiona la muestra M en subconjuntos **Mi**, uno por cada valor único de `event_day` (Lunes, Martes, etc.).\n",
        "\n",
        "## 1.4 Importancia de las particiones Mi\n",
        "\n",
        "Este particionamiento permite analizar el comportamiento de los usuarios en función del día de la semana, lo que facilita identificar patrones específicos (por ejemplo, mayor número de compras los fines de semana).\n",
        "\n",
        "Además, garantiza que los modelos construidos posteriormente no se vean sesgados por concentraciones excesivas de datos en ciertos días, y puedan generalizar mejor a lo largo de toda la semana.\n",
        "\n",
        "Cada partición **Mi** es mutuamente excluyente y la unión de todas ellas forma el conjunto completo **M**, cumpliendo con lo solicitado en la instrucción.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Librerías necesarias"
      ],
      "metadata": {
        "id": "4mCWOpZVCgS0"
      },
      "id": "4mCWOpZVCgS0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar Java (necesario para Spark)\n",
        "!apt-get install openjdk-11-jdk -y\n",
        "\n",
        "# Descargar e instalar Spark\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "!mv spark-3.3.2-bin-hadoop3 /opt/spark\n",
        "\n",
        "# Configurar las variables de entorno\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D-IS4klCeiz",
        "outputId": "bc15fa52-99df-4dc4-8fd8-b7866f024e59"
      },
      "id": "2D-IS4klCeiz",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jre\n",
            "  x11-utils\n",
            "Suggested packages:\n",
            "  libxt-doc openjdk-11-demo openjdk-11-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jdk\n",
            "  openjdk-11-jre x11-utils\n",
            "0 upgraded, 10 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 6,920 kB of archives.\n",
            "After this operation, 16.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.27+6~us1-0ubuntu1~22.04 [214 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.27+6~us1-0ubuntu1~22.04 [2,895 kB]\n",
            "Fetched 6,920 kB in 2s (3,427 kB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 126111 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../1-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../2-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../3-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../4-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../5-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../6-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../7-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../8-openjdk-11-jre_11.0.27+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../9-openjdk-11-jdk_11.0.27+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFtL8XuoDNfp",
        "outputId": "5b0339af-dd4a-432a-a4c3-e2eb51f3195d"
      },
      "id": "YFtL8XuoDNfp",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Test\").getOrCreate()\n",
        "spark.range(5).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3fsxIZkDWdq",
        "outputId": "1b75fbf8-f89a-4316-c680-ccebb987f891"
      },
      "id": "c3fsxIZkDWdq",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82006772-f8ee-47b7-8493-86d7313dbc17",
      "metadata": {
        "id": "82006772-f8ee-47b7-8493-86d7313dbc17"
      },
      "source": [
        "### Carga y filtrado de eventos relevantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "994a5dde-6e0c-4643-8ba0-1cc5efed9c66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "994a5dde-6e0c-4643-8ba0-1cc5efed9c66",
        "outputId": "302b3bb9-781c-4042-dcf6-1f798195a758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "import os\n",
        "\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--driver-memory 32g pyspark-shell\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Crear la sesión de Spark\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Actividad 4 - Spark estable en Colab Pro+\") \\\n",
        "    .config(\"spark.driver.memory\", \"12g\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Ruta del archivo\n",
        "dataset_path = \"/content/drive/MyDrive/Big Data/2019-Oct.csv\"\n",
        "\n",
        "\n",
        "# Cargar el dataset\n",
        "df = spark.read.csv(dataset_path, header=True, inferSchema=True)\n",
        "\n",
        "# Filtrar solo los eventos 'view', 'cart' y 'purchase'\n",
        "partitions = df.filter(\n",
        "    (col(\"event_type\") == \"view\") |\n",
        "    (col(\"event_type\") == \"cart\") |\n",
        "    (col(\"event_type\") == \"purchase\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a3c24c9-2a0e-4e86-96c6-fef51c6ca234",
      "metadata": {
        "id": "1a3c24c9-2a0e-4e86-96c6-fef51c6ca234"
      },
      "source": [
        "## Muestreo estratificado balanceado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dfb69c2e-ffa4-476c-bb08-f6b748994238",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfb69c2e-ffa4-476c-bb08-f6b748994238",
        "outputId": "12f08276-a327-416b-eeea-92f546fca5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+\n",
            "|event_type|   count|\n",
            "+----------+--------+\n",
            "|  purchase|  742849|\n",
            "|      cart|  926516|\n",
            "|      view|40779399|\n",
            "+----------+--------+\n",
            "\n",
            "Total de eventos balanceados: 2228547\n",
            "+----------+------+\n",
            "|event_type| count|\n",
            "+----------+------+\n",
            "|      view|742849|\n",
            "|      cart|742849|\n",
            "|  purchase|742849|\n",
            "+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Contar eventos por tipo\n",
        "event_type_counts = partitions.groupBy(\"event_type\").count()\n",
        "event_type_counts.show()\n",
        "\n",
        "# Obtener el tamaño mínimo entre las clases para balancear\n",
        "min_sample_size = min(row[\"count\"] for row in event_type_counts.collect())\n",
        "\n",
        "# Construir muestra balanceada\n",
        "balanced_df = (\n",
        "    partitions.filter(col(\"event_type\") == \"view\").limit(min_sample_size)\n",
        "    .union(partitions.filter(col(\"event_type\") == \"cart\").limit(min_sample_size))\n",
        "    .union(partitions.filter(col(\"event_type\") == \"purchase\").limit(min_sample_size))\n",
        ")\n",
        "\n",
        "# Verificar conteo\n",
        "print(\"Total de eventos balanceados:\", balanced_df.count())\n",
        "balanced_df.groupBy(\"event_type\").count().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpieza de datos\n",
        "Para garantizar que la muestra M no contenga ruido ni valores faltantes que afecten los modelos, se aplicó una limpieza de datos mediante la eliminación de registros con nulos y outliers extremos en la variable price. Además, se extrajeron atributos temporales a partir de event_time para utilizarse como criterios de particionamiento."
      ],
      "metadata": {
        "id": "DaZtL4DHstog"
      },
      "id": "DaZtL4DHstog"
    },
    {
      "cell_type": "markdown",
      "id": "cd785f04-b57f-4c70-ad0f-348cfb70cb47",
      "metadata": {
        "id": "cd785f04-b57f-4c70-ad0f-348cfb70cb47"
      },
      "source": [
        "## Eliminación de valores nulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "40bbcd3b-3748-470c-ac55-1c4b2b602447",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40bbcd3b-3748-470c-ac55-1c4b2b602447",
        "outputId": "2dcce6d6-8632-4a84-9ed5-9bdbb598e5c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eventos después de eliminar nulos: 1660655\n"
          ]
        }
      ],
      "source": [
        "# Eliminar registros con valores nulos\n",
        "balanced_df = balanced_df.na.drop()\n",
        "\n",
        "# Verificar tamaño después de limpieza\n",
        "print(\"Eventos después de eliminar nulos:\", balanced_df.count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2851ad5e-df54-4e92-b5db-2c87066ae328",
      "metadata": {
        "id": "2851ad5e-df54-4e92-b5db-2c87066ae328"
      },
      "source": [
        "## Elimicación de outliers en la variable Price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9acc2a6f-527f-450a-a5e4-ed9dc36e0504",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9acc2a6f-527f-450a-a5e4-ed9dc36e0504",
        "outputId": "887c3fb7-72f6-41d5-bdc5-4f40e046da0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eventos después de eliminar outliers en price: 1660655\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Obtener percentiles 1% y 99% para 'price'\n",
        "quantiles = balanced_df.approxQuantile(\"price\", [0.01, 0.99], 0.05)\n",
        "min_price, max_price = quantiles\n",
        "\n",
        "# Filtrar registros dentro del rango aceptable\n",
        "balanced_df = balanced_df.filter((col(\"price\") >= min_price) & (col(\"price\") <= max_price))\n",
        "\n",
        "# Verificar conteo\n",
        "print(\"Eventos después de eliminar outliers en price:\", balanced_df.count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a947fb7-4541-47bc-824e-8952f625538e",
      "metadata": {
        "id": "7a947fb7-4541-47bc-824e-8952f625538e"
      },
      "source": [
        "## Converisón de variables de tiempo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "97d53060-1904-4302-953e-3cd312e1f3e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97d53060-1904-4302-953e-3cd312e1f3e8",
        "outputId": "df086142-eb48-4f52-aa5a-d3ebf726659c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+----------+----------+---------+\n",
            "|event_time         |event_time_ts      |event_date|event_hour|event_day|\n",
            "+-------------------+-------------------+----------+----------+---------+\n",
            "|2019-10-01 00:00:00|2019-10-01 00:00:00|2019-10-01|0         |Tue      |\n",
            "|2019-10-01 00:00:01|2019-10-01 00:00:01|2019-10-01|0         |Tue      |\n",
            "|2019-10-01 00:00:04|2019-10-01 00:00:04|2019-10-01|0         |Tue      |\n",
            "|2019-10-01 00:00:05|2019-10-01 00:00:05|2019-10-01|0         |Tue      |\n",
            "|2019-10-01 00:00:10|2019-10-01 00:00:10|2019-10-01|0         |Tue      |\n",
            "+-------------------+-------------------+----------+----------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import to_timestamp, date_format\n",
        "\n",
        "# Convertir 'event_time' a timestamp\n",
        "balanced_df = balanced_df.withColumn(\"event_time_ts\", to_timestamp(\"event_time\", \"yyyy-MM-dd HH:mm:ss\"))\n",
        "\n",
        "# Extraer componentes temporales\n",
        "balanced_df = balanced_df.withColumn(\"event_date\", date_format(\"event_time_ts\", \"yyyy-MM-dd\"))\n",
        "balanced_df = balanced_df.withColumn(\"event_hour\", date_format(\"event_time_ts\", \"HH\").cast(\"int\"))\n",
        "balanced_df = balanced_df.withColumn(\"event_day\", date_format(\"event_time_ts\", \"E\"))\n",
        "\n",
        "# Mostrar ejemplo\n",
        "balanced_df.select(\"event_time\", \"event_time_ts\", \"event_date\", \"event_hour\", \"event_day\").show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d82f817a-d5aa-43fc-b0af-38848b8a9cb5",
      "metadata": {
        "id": "d82f817a-d5aa-43fc-b0af-38848b8a9cb5"
      },
      "source": [
        "## Generación de particiones Mi por el día en que ha ocurrido el evento: event_day\n",
        "\n",
        "Para evitar sesgos derivados de una distribución desigual de eventos según el día de la semana (event_day), se generaron particiones Mi balanceadas con un mismo número de registros por día. Posteriormente, se unieron todas las Mi para formar la muestra representativa M. Esto garantiza equidad en la representación de las distintas condiciones temporales en el modelo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d0852c98-d980-4c2d-9a62-abe32c87b94f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0852c98-d980-4c2d-9a62-abe32c87b94f",
        "outputId": "e91e34e0-4750-454d-ec2d-cc857e9593ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Días encontrados: ['Tue', 'Wed', 'Sun', 'Sat', 'Thu', 'Mon', 'Fri']\n",
            "Número mínimo de instancias por día (para balancear): 154482\n",
            "Total de registros en la muestra M balanceada: 1081374\n",
            "+---------+------+\n",
            "|event_day| count|\n",
            "+---------+------+\n",
            "|      Tue|154482|\n",
            "|      Wed|154482|\n",
            "|      Sun|154482|\n",
            "|      Sat|154482|\n",
            "|      Thu|154482|\n",
            "|      Mon|154482|\n",
            "|      Fri|154482|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from functools import reduce\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "# Verificar los días únicos en el dataset balanceado\n",
        "dias_disponibles = [row[\"event_day\"] for row in balanced_df.select(\"event_day\").distinct().collect()]\n",
        "print(\"Días encontrados:\", dias_disponibles)\n",
        "\n",
        "# Crear un diccionario para almacenar las particiones Mi\n",
        "particiones_mi = {}\n",
        "\n",
        "# Obtener el número mínimo de registros por día para balancear\n",
        "conteo_por_dia = balanced_df.groupBy(\"event_day\").count()\n",
        "minimo_por_dia = conteo_por_dia.agg({\"count\": \"min\"}).collect()[0][0]\n",
        "print(f\"Número mínimo de instancias por día (para balancear): {minimo_por_dia}\")\n",
        "\n",
        "# Crear una partición Mi para cada día con igual número de registros\n",
        "for dia in dias_disponibles:\n",
        "    particiones_mi[dia] = balanced_df.filter(col(\"event_day\") == dia).limit(minimo_por_dia)\n",
        "\n",
        "# Unir todas las particiones para formar M final\n",
        "muestra_m = reduce(DataFrame.unionAll, particiones_mi.values())\n",
        "\n",
        "# Verificar que M es la unión de todos los Mi\n",
        "print(\"Total de registros en la muestra M balanceada:\", muestra_m.count())\n",
        "muestra_m.groupBy(\"event_day\").count().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76bbc3fd-ad02-4e3a-bfd5-017c2da1ee0f",
      "metadata": {
        "id": "76bbc3fd-ad02-4e3a-bfd5-017c2da1ee0f"
      },
      "source": [
        "# 2. Construcción Train – Test\n",
        "\n",
        "## División y preprocesamiento por partición Mi (`event_day`)\n",
        "\n",
        "Este bloque de código realiza la división de cada partición `Mi` (agrupadas por `event_day`) en conjuntos de entrenamiento (`Tri`) y prueba (`Tsi`), y luego aplica transformaciones por separado para cada uno.\n",
        "\n",
        "### Objetivo\n",
        "\n",
        "- Mantener una representación balanceada de los datos por día.\n",
        "- Aplicar estandarización y codificación **sin filtrar información del conjunto de prueba**, es decir, evitando *data leakage*.\n",
        "\n",
        "### Preprocesamiento aplicado\n",
        "\n",
        "1. **Estandarización (Z-score)** del atributo numérico `price` usando `StandardScaler`:\n",
        "   - Se utiliza `VectorAssembler` para ensamblar `price` en un vector.\n",
        "   - `StandardScaler` se ajusta (`fit`) **solo con `Tri`** para calcular media y desviación estándar.\n",
        "   - Luego se aplica (`transform`) tanto a `Tri` como a `Tsi`.\n",
        "\n",
        "2. **Codificación de variables categóricas** usando `StringIndexer`:\n",
        "   - También se ajusta (`fit`) **solo con `Tri`**.\n",
        "   - Se aplica (`transform`) en ambos conjuntos (`Tri` y `Tsi`) para mantener coherencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a64b3b3-2041-4cf2-9c63-8729eb750dfd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a64b3b3-2041-4cf2-9c63-8729eb750dfd",
        "outputId": "1939e09e-2352-4f6b-857e-170baf2e6c83"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Procesando día: Tue\n",
            "\n",
            " Procesando día: Wed\n",
            "\n",
            " Procesando día: Sun\n",
            "\n",
            " Procesando día: Sat\n",
            "\n",
            " Procesando día: Thu\n",
            "\n",
            " Procesando día: Mon\n",
            "\n",
            " Procesando día: Fri\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from functools import reduce\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
        "\n",
        "# Inicializar listas para almacenar Tri y Tsi\n",
        "train_scaled_list = []\n",
        "test_scaled_list = []\n",
        "train_unscaled_list = []\n",
        "test_unscaled_list = []\n",
        "\n",
        "# Parámetros de división\n",
        "train_ratio = 0.8\n",
        "test_ratio = 0.2\n",
        "seed = 42\n",
        "\n",
        "# Columnas a codificar\n",
        "columns_to_index = [\n",
        "    \"event_type\", \"event_day\", \"event_hour\", \"event_date\",\n",
        "    \"category_code\", \"brand\", \"product_id\", \"user_id\"\n",
        "]\n",
        "\n",
        "# División y procesamiento por partición Mi (día)\n",
        "for dia, mi_df in particiones_mi.items():\n",
        "    print(f\"\\n Procesando día: {dia}\")\n",
        "\n",
        "    # === División de la partición Mi ===\n",
        "    tri_raw, tsi_raw = mi_df.randomSplit([train_ratio, test_ratio], seed=seed)\n",
        "\n",
        "    # === ESCALADO ===\n",
        "    assembler = VectorAssembler(inputCols=[\"price\"], outputCol=\"price_vector\")\n",
        "    tri_vec = assembler.transform(tri_raw)\n",
        "    tsi_vec = assembler.transform(tsi_raw)\n",
        "\n",
        "    scaler = StandardScaler(inputCol=\"price_vector\", outputCol=\"price_scaled\", withMean=True, withStd=True)\n",
        "    scaler_model = scaler.fit(tri_vec)\n",
        "\n",
        "    tri_scaled = scaler_model.transform(tri_vec)\n",
        "    tsi_scaled = scaler_model.transform(tsi_vec)\n",
        "\n",
        "    # === CODIFICACIÓN ===\n",
        "    for col_name in columns_to_index:\n",
        "        indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_indexed\")\n",
        "        indexer_model = indexer.fit(tri_scaled)\n",
        "        tri_scaled = indexer_model.transform(tri_scaled)\n",
        "        tsi_scaled = indexer_model.transform(tsi_scaled)\n",
        "\n",
        "    # === Versión sin escalar (solo codificación) ===\n",
        "    tri_unscaled = tri_raw\n",
        "    tsi_unscaled = tsi_raw\n",
        "\n",
        "    for col_name in columns_to_index:\n",
        "        indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_indexed\")\n",
        "        indexer_model = indexer.fit(tri_unscaled)\n",
        "        tri_unscaled = indexer_model.transform(tri_unscaled)\n",
        "        tsi_unscaled = indexer_model.transform(tsi_unscaled)\n",
        "\n",
        "    # === Acumular en listas globales ===\n",
        "    train_scaled_list.append(tri_scaled)\n",
        "    test_scaled_list.append(tsi_scaled)\n",
        "    train_unscaled_list.append(tri_unscaled)\n",
        "    test_unscaled_list.append(tsi_unscaled)\n",
        "\n",
        "# === Unión global ===\n",
        "train_scaled = reduce(DataFrame.unionAll, train_scaled_list)\n",
        "test_scaled = reduce(DataFrame.unionAll, test_scaled_list)\n",
        "train_unscaled = reduce(DataFrame.unionAll, train_unscaled_list)\n",
        "test_unscaled = reduce(DataFrame.unionAll, test_unscaled_list)\n",
        "\n",
        "# === Validaciones ===\n",
        "print(\"Ejemplos (Train escalado):\")\n",
        "train_scaled.select(\"event_type\", \"price\", \"price_scaled\").limit(5).show(truncate=False)\n",
        "\n",
        "print(\" Ejemplos (Test no escalado):\")\n",
        "test_unscaled.select(\"event_type\", \"price\").limit(5).show(truncate=False)\n",
        "\n",
        "print(\" Columnas (escalado):\", train_scaled.columns)\n",
        "print(\"Columnas (no escalado):\", train_unscaled.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8f2b5b5-5cdf-4f4c-9ec8-6dff2acd4be9",
      "metadata": {
        "id": "e8f2b5b5-5cdf-4f4c-9ec8-6dff2acd4be9"
      },
      "source": [
        "## Validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d16b3b46-29b9-4e24-a414-1d4bf7cad821",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d16b3b46-29b9-4e24-a414-1d4bf7cad821",
        "outputId": "5219c1bf-077e-4bad-fdbf-b3b39d7016ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Conteo total de registros ===\n",
            "Train (escalado): 864031 registros\n",
            "Test  (escalado): 217343 registros\n",
            "Train (no escalado): 864031 registros\n",
            "Test  (no escalado): 217343 registros\n",
            "\n",
            "=== Distribución de clases por conjunto ===\n",
            "\n",
            "Train (escalado):\n",
            "+----------+------+\n",
            "|event_type| count|\n",
            "+----------+------+\n",
            "|      cart|432998|\n",
            "|  purchase|307600|\n",
            "|      view|123433|\n",
            "+----------+------+\n",
            "\n",
            "\n",
            "Test (escalado):\n",
            "+----------+------+\n",
            "|event_type| count|\n",
            "+----------+------+\n",
            "|      cart|108740|\n",
            "|  purchase| 77554|\n",
            "|      view| 31049|\n",
            "+----------+------+\n",
            "\n",
            "\n",
            "Train (no escalado):\n",
            "+----------+------+\n",
            "|event_type| count|\n",
            "+----------+------+\n",
            "|      cart|432998|\n",
            "|  purchase|307600|\n",
            "|      view|123433|\n",
            "+----------+------+\n",
            "\n",
            "\n",
            "Test (no escalado):\n",
            "+----------+------+\n",
            "|event_type| count|\n",
            "+----------+------+\n",
            "|      cart|108740|\n",
            "|  purchase| 77554|\n",
            "|      view| 31049|\n",
            "+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === Conteo total por conjunto ===\n",
        "print(\"=== Conteo total de registros ===\")\n",
        "print(f\"Train (escalado): {train_scaled.count()} registros\")\n",
        "print(f\"Test  (escalado): {test_scaled.count()} registros\")\n",
        "print(f\"Train (no escalado): {train_unscaled.count()} registros\")\n",
        "print(f\"Test  (no escalado): {test_unscaled.count()} registros\")\n",
        "\n",
        "# === Conteo por clase: event_type ===\n",
        "print(\"\\n=== Distribución de clases por conjunto ===\")\n",
        "\n",
        "print(\"\\nTrain (escalado):\")\n",
        "train_scaled.groupBy(\"event_type\").count().orderBy(\"event_type\").show()\n",
        "\n",
        "print(\"\\nTest (escalado):\")\n",
        "test_scaled.groupBy(\"event_type\").count().orderBy(\"event_type\").show()\n",
        "\n",
        "print(\"\\nTrain (no escalado):\")\n",
        "train_unscaled.groupBy(\"event_type\").count().orderBy(\"event_type\").show()\n",
        "\n",
        "print(\"\\nTest (no escalado):\")\n",
        "test_unscaled.groupBy(\"event_type\").count().orderBy(\"event_type\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fb72006-0ba1-4421-b391-504bc0434204",
      "metadata": {
        "id": "8fb72006-0ba1-4421-b391-504bc0434204"
      },
      "source": [
        "## 3. Selección de métricas para medir calidad de resultados\n",
        "\n",
        "Para evaluar adecuadamente los modelos de aprendizaje generados, es fundamental seleccionar métricas que se adapten a la naturaleza del problema, considerando también el gran volumen de datos procesados.\n",
        "\n",
        "En este caso se han implementado dos enfoques de aprendizaje:\n",
        "- **Supervisado**, mediante un modelo de clasificación multiclase (Random Forest).\n",
        "- **No supervisado**, mediante agrupamiento (clustering) con KMeans.\n",
        "\n",
        "A continuación, se detallan las métricas seleccionadas para cada enfoque.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.1 Métricas para el modelo supervisado (Random Forest)\n",
        "\n",
        "El objetivo del modelo supervisado es predecir el tipo de evento de usuario (`view`, `cart`, `purchase`), lo cual es un problema de **clasificación multiclase**. Por tanto, se seleccionan las siguientes métricas:\n",
        "\n",
        "#### 1. **Accuracy (Precisión global)**\n",
        "\n",
        "Proporción de predicciones correctas sobre el total de observaciones. Es útil como una visión general del rendimiento del modelo.\n",
        "\n",
        "> **Fórmula:**  \n",
        "> `Accuracy = (TP + TN) / (TP + FP + FN + TN)`\n",
        "\n",
        "#### 2. **Precision (Precisión ponderada)**\n",
        "\n",
        "Evalúa cuántas de las predicciones positivas fueron correctas. Se utiliza la versión ponderada que tiene en cuenta el soporte (frecuencia relativa) de cada clase.\n",
        "\n",
        "> **Fórmula:**  \n",
        "> `Precision = TP / (TP + FP)`\n",
        "\n",
        "#### 3. **Recall (Sensibilidad ponderada)**\n",
        "\n",
        "Mide cuántos verdaderos positivos fueron correctamente identificados. Se pondera para reflejar adecuadamente el impacto de cada clase.\n",
        "\n",
        "> **Fórmula:**  \n",
        "> `Recall = TP / (TP + FN)`\n",
        "\n",
        "#### 4. **F1-Score (Promedio armónico ponderado)**\n",
        "\n",
        "Combina precisión y recall. Es especialmente útil cuando hay desbalance o se necesita equilibrio entre ambas métricas.\n",
        "\n",
        "> **Fórmula:**  \n",
        "> `F1 = 2 * (Precision * Recall) / (Precision + Recall)`\n",
        "\n",
        "#### Justificación:\n",
        "- Todas las métricas son escalables en PySpark.\n",
        "- La versión ponderada permite mantener validez incluso en presencia de desbalance leve.\n",
        "- Estas métricas facilitan la comparación con otros modelos y algoritmos supervisados.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 Métricas para el modelo no supervisado (KMeans)\n",
        "\n",
        "El modelo de agrupamiento no cuenta con etiquetas reales, por lo que se utilizan métricas que evalúan la **calidad de la segmentación** de los datos.\n",
        "\n",
        "#### 1. **Silhouette Score**\n",
        "\n",
        "Mide la calidad de la separación entre clusters considerando cohesión interna y separación externa. Un valor cercano a 1 indica que los puntos están bien agrupados y separados de otros clusters.\n",
        "\n",
        "> **Valor esperado:** entre -1 y 1  \n",
        "> Un valor > 0.5 es generalmente aceptable.\n",
        "\n",
        "#### 2. **SSE (Suma de errores cuadráticos)**\n",
        "\n",
        "También conocida como **inercia**, mide la compactación de los clusters. Se utiliza para aplicar el método del codo (\"elbow method\") y seleccionar el número óptimo de clusters `k`.\n",
        "\n",
        "> **Fórmula:**  \n",
        "> `SSE = sum((p_i - c_j)^2)` para cada punto `p_i` y su centroide `c_j`\n",
        "\n",
        "#### Justificación:\n",
        "- Ambas métricas permiten medir la calidad sin necesidad de etiquetas.\n",
        "- El Silhouette Score proporciona una medida relativa de desempeño entre múltiples ejecuciones con diferente `k`.\n",
        "- El SSE es útil para detectar el punto de inflexión donde agregar más clusters no mejora significativamente el modelo.\n",
        "- Son eficientes y escalables en PySpark.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusión general\n",
        "\n",
        "Se utilizarán métricas distintas según el tipo de modelo implementado:\n",
        "\n",
        "| Tipo de Modelo   | Métricas Seleccionadas                          |\n",
        "|------------------|-------------------------------------------------|\n",
        "| Supervisado      | Accuracy, Precision (weighted), Recall (weighted), F1-Score (weighted) |\n",
        "| No supervisado   | Silhouette Score, SSE (inercia)                |\n",
        "\n",
        "Estas métricas permiten evaluar correctamente el desempeño de los modelos en el contexto de Big Data, maximizando la utilidad de los resultados para identificar patrones significativos en los datos de comportamiento de usuarios.\n",
        "\n",
        "Todas las métricas fueron seleccionadas no solo por su relevancia conceptual, sino también por su capacidad de ser computadas eficientemente en entornos de Big Data mediante PySpark, asegurando su viabilidad técnica durante la experimentación con millones de registros.\n",
        "---\n",
        "\n",
        "## Referencias\n",
        "1. **Apache Spark.** (2025). *DecisionTreeClassifier — PySpark 3.5.5 documentation*. Recuperado de [Apache Spark Documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.DecisionTreeClassifier.html)\n",
        "2. **Apache Spark.** (2025). *KMeans — PySpark 3.5.5 documentation*. Recuperado de [Apache Spark Documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.KMeans.html)\n",
        "3. **Apache Spark.** (2025). *Clustering - Spark 3.5.5 Documentation*. Recuperado de [Apache Spark Documentation](https://spark.apache.org/docs/latest/ml-clustering.html)\n",
        "4. **Apache Spark.** (2025). *Classification and regression - Spark 3.5.5 Documentation*. Recuperado de [Apache Spark Documentation](https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
        "5. **GeeksforGeeks.** (2025). *K-Means Clustering using PySpark Python*. Recuperado de [GeeksforGeeks](https://www.geeksforgeeks.org/k-means-clustering-using-pyspark-python/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d51c40-2c43-4f9d-b684-4173e47739d4",
      "metadata": {
        "id": "20d51c40-2c43-4f9d-b684-4173e47739d4"
      },
      "source": [
        "## 4. Entrenamiento de Modelos de Aprendizaje\n",
        "\n",
        "En esta sección se implementan dos modelos de aprendizaje: uno supervisado y uno no supervisado, con el objetivo de identificar patrones de comportamiento en grandes volúmenes de datos derivados del entorno de e-commerce.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.1 Modelo Supervisado: Random Forest Classifier\n",
        "\n",
        "Se seleccionó el modelo **Random Forest** como clasificador supervisado por su robustez ante datos ruidosos, su capacidad para manejar grandes volúmenes de datos y su buen desempeño incluso con pocas transformaciones previas. Se probó con diferentes combinaciones de hiperparámetros para mejorar el rendimiento.\n",
        "\n",
        "#### Características del modelo:\n",
        "- **Tipo**: Supervisado\n",
        "- **Algoritmo**: Random Forest Classifier\n",
        "- **Librería**: PySpark MLlib\n",
        "\n",
        "#### Variables objetivo y predictoras:\n",
        "- **Target (`label`)**: `event_type_indexed` (codificado como view, cart, purchase)\n",
        "- **Features**: `price_scaled`, `event_day_indexed`, `event_hour_indexed`\n",
        "\n",
        "#### Hiperparámetros utilizados:\n",
        "| Hiperparámetro           | Valor                 | Descripción                                       |\n",
        "|--------------------------|-----------------------|---------------------------------------------------|\n",
        "| `numTrees`               | 100, 200              | Número de árboles en el bosque                   |\n",
        "| `maxDepth`               | 10, 15, 20            | Profundidad máxima de cada árbol                 |\n",
        "| `minInstancesPerNode`    | 5 (opcional)          | Mínimo de instancias por nodo para regularización|\n",
        "| `featureSubsetStrategy`  | `\"sqrt\"` (opcional)   | Subconjunto aleatorio de características por árbol|\n",
        "| `maxBins`                | 2000 (en algunos casos)| Número de divisiones para variables continuas    |\n",
        "| `seed`                   | 42                    | Reproducibilidad del experimento                 |\n",
        "\n",
        "#### Procesamiento:\n",
        "- Los datos fueron balanceados por clase y divididos en subconjuntos \\( M_i \\) por día.\n",
        "- Se aplicó codificación consistente a variables categóricas.\n",
        "- Se ensamblaron las variables con `VectorAssembler` para formar la columna `features`.\n",
        "- Se dividió el conjunto en entrenamiento y prueba (80/20).\n",
        "- Se evaluaron distintas configuraciones para identificar la mejor combinación de hiperparámetros.\n",
        "\n",
        "#### Prevención de sobreajuste:\n",
        "- Se limitaron la profundidad de los árboles (`maxDepth`) y el número mínimo de instancias por nodo (`minInstancesPerNode`).\n",
        "- Se utilizó `featureSubsetStrategy` para aumentar la diversidad de árboles.\n",
        "- Las métricas se compararon entre entrenamiento y prueba para validar la generalización.\n",
        "\n",
        "#### Métricas utilizadas:\n",
        "- **Accuracy**\n",
        "- **Precision**\n",
        "- **Recall**\n",
        "- **F1-score**\n",
        "\n",
        "Estas métricas permitieron evaluar el desempeño global, el balance entre clases y la capacidad de identificar correctamente eventos importantes como \"purchase\".\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2 Modelo No Supervisado: Bisecting KMeans\n",
        "\n",
        "Como modelo no supervisado se seleccionó **BisectingKMeans**, una variante jerárquica de KMeans que divide los datos recursivamente para formar clústeres. Se eligió por su escalabilidad, buena separación y eficiencia sobre grandes volúmenes de datos.\n",
        "\n",
        "#### Características del modelo:\n",
        "- **Tipo**: No supervisado\n",
        "- **Algoritmo**: Bisecting KMeans Clustering\n",
        "- **Librería**: PySpark MLlib\n",
        "\n",
        "#### Variables utilizadas:\n",
        "- `price_scaled`\n",
        "- `event_day_indexed`\n",
        "- `event_hour_indexed`\n",
        "\n",
        "#### Hiperparámetros evaluados:\n",
        "| Hiperparámetro           | Valor probado       | Descripción                                      |\n",
        "|--------------------------|---------------------|--------------------------------------------------|\n",
        "| `k`                      | 2, 3, 4, 5           | Número de clústeres                              |\n",
        "| `maxIter`                | 50                   | Máximo de iteraciones por partición              |\n",
        "| `seed`                   | 42                   | Semilla para reproducibilidad                    |\n",
        "\n",
        "#### Estrategia de entrenamiento:\n",
        "- Se generaron vectores de características con `VectorAssembler`.\n",
        "- Se entrenaron modelos con diferentes valores de `k` (número de clústeres).\n",
        "- Se aplicó evaluación usando **Silhouette Score** y **SSE** (Suma de Errores Cuadráticos) para comparar agrupaciones.\n",
        "- Se comparó el desempeño entre el conjunto de entrenamiento y prueba para verificar la estabilidad.\n",
        "\n",
        "#### Prevención de sobreajuste:\n",
        "- Se evitó usar valores de `k` demasiado grandes que generaran clústeres artificiales.\n",
        "- Se comparó la estabilidad del Silhouette Score entre entrenamiento y prueba.\n",
        "\n",
        "#### Métricas utilizadas:\n",
        "- **Silhouette Score**\n",
        "- **SSE (Sum of Squared Errors)**\n",
        "\n",
        "Estas métricas permitieron validar la calidad de los agrupamientos generados, observando la cohesión y separación entre clústeres en el espacio vectorial.\n",
        "\n",
        "---\n",
        "\n",
        "En ambos enfoques se garantizó la reproducibilidad, el uso de datos balanceados por clase, y se documentó cada paso del entrenamiento para asegurar que los modelos fueran confiables, interpretables y aplicables a un contexto real de análisis de comportamiento en plataformas de comercio electrónico.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1377e2cc-5414-44bd-a036-e8589cd19045",
      "metadata": {
        "id": "1377e2cc-5414-44bd-a036-e8589cd19045"
      },
      "source": [
        "## Modelo supervisado RandomForest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reindexación de variables categóricas\n",
        "\n",
        "En esta celda se realiza una reindexación completa de las variables categóricas para asegurar la consistencia de los valores indexados en todos los conjuntos de datos (`train_scaled`, `test_scaled`, `train_unscaled`, `test_unscaled`).\n",
        "\n",
        "#### ¿Por qué es necesario este paso?\n",
        "\n",
        "Durante el preprocesamiento inicial, los índices de las variables categóricas se generaron por separado en cada conjunto o partición. Esto puede ocasionar inconsistencias, por ejemplo:\n",
        "\n",
        "- La categoría `purchase` podría recibir el índice `0` en `train_scaled`, pero `1` en `test_scaled`.\n",
        "- Esto causa errores durante el entrenamiento de modelos que asumen consistencia en los valores de entrada, como los clasificadores basados en árboles.\n",
        "\n",
        "#### ¿Qué se hace en esta celda?\n",
        "\n",
        "1. Se eliminan las columnas `*_indexed` existentes, si las hubiera, para evitar conflictos.\n",
        "2. Se entrena un nuevo `StringIndexer` sobre `train_scaled` para cada variable categórica.\n",
        "3. Los modelos `StringIndexer` se aplican a todos los conjuntos (`train_scaled`, `test_scaled`, `train_unscaled`, `test_unscaled`) para mantener los mismos índices en todos.\n",
        "4. Se establece el parámetro `handleInvalid=\"keep\"` para evitar errores en caso de que aparezcan categorías no vistas durante el entrenamiento del indexador.\n",
        "\n",
        "Este proceso asegura que los modelos de machine learning reciban entradas coherentes, lo cual es fundamental para obtener resultados confiables y evitar errores de ejecución.\n"
      ],
      "metadata": {
        "id": "gZ0KhngPeBb8"
      },
      "id": "gZ0KhngPeBb8"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Columnas categóricas a codificar\n",
        "columns_to_index = [\n",
        "    \"event_type\", \"event_day\", \"event_hour\", \"event_date\",\n",
        "    \"category_code\", \"brand\", \"product_id\", \"user_id\"\n",
        "]\n",
        "\n",
        "# Eliminar columnas indexadas existentes si ya estaban\n",
        "for col in columns_to_index:\n",
        "    indexed_col = f\"{col}_indexed\"\n",
        "    for df_name in [\"train_scaled\", \"test_scaled\", \"train_unscaled\", \"test_unscaled\"]:\n",
        "        df = globals()[df_name]\n",
        "        if indexed_col in df.columns:\n",
        "            df = df.drop(indexed_col)\n",
        "        globals()[df_name] = df  # Actualiza variable global\n",
        "\n",
        "# Aplicar StringIndexer desde train_scaled y transformar en todos\n",
        "for col in columns_to_index:\n",
        "    indexed_col = f\"{col}_indexed\"\n",
        "    indexer = StringIndexer(inputCol=col, outputCol=indexed_col, handleInvalid=\"keep\")\n",
        "    model = indexer.fit(train_scaled)\n",
        "\n",
        "    train_scaled = model.transform(train_scaled)\n",
        "    test_scaled = model.transform(test_scaled)\n",
        "    train_unscaled = model.transform(train_unscaled)\n",
        "    test_unscaled = model.transform(test_unscaled)\n"
      ],
      "metadata": {
        "id": "VmxSmo7UVYa7"
      },
      "id": "VmxSmo7UVYa7",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "82598fb9-7195-413c-872b-4129d4b4388e",
      "metadata": {
        "id": "82598fb9-7195-413c-872b-4129d4b4388e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7368b694-98e1-4297-a7a4-8b165285a838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparación de métricas ---\n",
            "Accuracy:\n",
            "  Train: 0.6537\n",
            "  Test : 0.6500\n",
            "Precision:\n",
            "  Train: 0.6408\n",
            "  Test : 0.6338\n",
            "Recall:\n",
            "  Train: 0.6537\n",
            "  Test : 0.6500\n",
            "F1-score:\n",
            "  Train: 0.5884\n",
            "  Test : 0.5842\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# === Ensamblador de características ===\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"price_scaled\", \"event_day_indexed\", \"event_hour_indexed\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# === Definición del modelo Random Forest con hiperparámetros ===\n",
        "rf = RandomForestClassifier(\n",
        "    labelCol=\"event_type_indexed\",   # Columna objetivo\n",
        "    featuresCol=\"features\",          # Columna de características\n",
        "    numTrees=100,                    # Hiperparámetro: número de árboles\n",
        "    maxDepth=15,                     # Hiperparámetro: profundidad máxima\n",
        "    seed=42                          # Hiperparámetro: reproducibilidad\n",
        ")\n",
        "\n",
        "# === Pipeline de ensamblado y modelo ===\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# === Entrenamiento del modelo ===\n",
        "model = pipeline.fit(train_scaled)\n",
        "\n",
        "# === Evaluación ===\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"event_type_indexed\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "\n",
        "# === Predicciones y métricas en conjunto de entrenamiento ===\n",
        "train_preds = model.transform(train_scaled)\n",
        "train_metrics = {\n",
        "    \"Accuracy\": evaluator.setMetricName(\"accuracy\").evaluate(train_preds),\n",
        "    \"Precision\": evaluator.setMetricName(\"weightedPrecision\").evaluate(train_preds),\n",
        "    \"Recall\": evaluator.setMetricName(\"weightedRecall\").evaluate(train_preds),\n",
        "    \"F1-score\": evaluator.setMetricName(\"f1\").evaluate(train_preds)\n",
        "}\n",
        "\n",
        "# === Predicciones y métricas en conjunto de prueba ===\n",
        "test_preds = model.transform(test_scaled)\n",
        "test_metrics = {\n",
        "    \"Accuracy\": evaluator.setMetricName(\"accuracy\").evaluate(test_preds),\n",
        "    \"Precision\": evaluator.setMetricName(\"weightedPrecision\").evaluate(test_preds),\n",
        "    \"Recall\": evaluator.setMetricName(\"weightedRecall\").evaluate(test_preds),\n",
        "    \"F1-score\": evaluator.setMetricName(\"f1\").evaluate(test_preds)\n",
        "}\n",
        "\n",
        "# === Mostrar comparación ===\n",
        "print(\"\\n--- Comparación de métricas ---\")\n",
        "for metric in train_metrics:\n",
        "    print(f\"{metric}:\")\n",
        "    print(f\"  Train: {train_metrics[metric]:.4f}\")\n",
        "    print(f\"  Test : {test_metrics[metric]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# === Ensamblador de características ===\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"price_scaled\", \"event_day_indexed\", \"event_hour_indexed\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# === Definición del modelo Random Forest con hiperparámetros ajustados ===\n",
        "rf = RandomForestClassifier(\n",
        "    labelCol=\"event_type_indexed\",\n",
        "    featuresCol=\"features\",\n",
        "    numTrees=200,                   # Aumentamos número de árboles\n",
        "    maxDepth=20,                    # Profundidad moderadamente alta\n",
        "    minInstancesPerNode=5,          # Evita árboles muy específicos\n",
        "    featureSubsetStrategy=\"sqrt\",   # Reduce correlación entre árboles\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# === Pipeline de ensamblado y modelo ===\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# === Entrenamiento del modelo ===\n",
        "model = pipeline.fit(train_scaled)\n",
        "\n",
        "# === Evaluador ===\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"event_type_indexed\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "\n",
        "# === Evaluación en conjunto de entrenamiento ===\n",
        "train_preds = model.transform(train_scaled)\n",
        "train_metrics = {\n",
        "    \"Accuracy\": evaluator.setMetricName(\"accuracy\").evaluate(train_preds),\n",
        "    \"Precision\": evaluator.setMetricName(\"weightedPrecision\").evaluate(train_preds),\n",
        "    \"Recall\": evaluator.setMetricName(\"weightedRecall\").evaluate(train_preds),\n",
        "    \"F1-score\": evaluator.setMetricName(\"f1\").evaluate(train_preds)\n",
        "}\n",
        "\n",
        "# === Evaluación en conjunto de prueba ===\n",
        "test_preds = model.transform(test_scaled)\n",
        "test_metrics = {\n",
        "    \"Accuracy\": evaluator.setMetricName(\"accuracy\").evaluate(test_preds),\n",
        "    \"Precision\": evaluator.setMetricName(\"weightedPrecision\").evaluate(test_preds),\n",
        "    \"Recall\": evaluator.setMetricName(\"weightedRecall\").evaluate(test_preds),\n",
        "    \"F1-score\": evaluator.setMetricName(\"f1\").evaluate(test_preds)\n",
        "}\n",
        "\n",
        "# === Mostrar comparación ===\n",
        "print(\"\\n--- Comparación de métricas ---\")\n",
        "for metric in train_metrics:\n",
        "    print(f\"{metric}:\")\n",
        "    print(f\"  Train: {train_metrics[metric]:.4f}\")\n",
        "    print(f\"  Test : {test_metrics[metric]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psdl_fbzq0FJ",
        "outputId": "21b31824-cba6-48f2-d349-5c87a1574e4a"
      },
      "id": "Psdl_fbzq0FJ",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparación de métricas ---\n",
            "Accuracy:\n",
            "  Train: 0.6547\n",
            "  Test : 0.6501\n",
            "Precision:\n",
            "  Train: 0.6415\n",
            "  Test : 0.6332\n",
            "Recall:\n",
            "  Train: 0.6547\n",
            "  Test : 0.6501\n",
            "F1-score:\n",
            "  Train: 0.5956\n",
            "  Test : 0.5903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# === Ensamblador de características ===\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"price_scaled\", \"event_day_indexed\", \"event_hour_indexed\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# === Definición del modelo Random Forest con hiperparámetros ajustados ===\n",
        "rf = RandomForestClassifier(\n",
        "    labelCol=\"event_type_indexed\",\n",
        "    featuresCol=\"features\",\n",
        "    numTrees=100,\n",
        "    maxDepth=10,\n",
        "    maxBins=2000,\n",
        "    impurity=\"gini\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# === Pipeline de ensamblado y modelo ===\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# === Entrenamiento del modelo ===\n",
        "model = pipeline.fit(train_scaled)\n",
        "\n",
        "# === Evaluador ===\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"event_type_indexed\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "\n",
        "# === Evaluación en conjunto de entrenamiento ===\n",
        "train_preds = model.transform(train_scaled)\n",
        "train_metrics = {\n",
        "    \"Accuracy\": evaluator.setMetricName(\"accuracy\").evaluate(train_preds),\n",
        "    \"Precision\": evaluator.setMetricName(\"weightedPrecision\").evaluate(train_preds),\n",
        "    \"Recall\": evaluator.setMetricName(\"weightedRecall\").evaluate(train_preds),\n",
        "    \"F1-score\": evaluator.setMetricName(\"f1\").evaluate(train_preds)\n",
        "}\n",
        "\n",
        "# === Evaluación en conjunto de prueba ===\n",
        "test_preds = model.transform(test_scaled)\n",
        "test_metrics = {\n",
        "    \"Accuracy\": evaluator.setMetricName(\"accuracy\").evaluate(test_preds),\n",
        "    \"Precision\": evaluator.setMetricName(\"weightedPrecision\").evaluate(test_preds),\n",
        "    \"Recall\": evaluator.setMetricName(\"weightedRecall\").evaluate(test_preds),\n",
        "    \"F1-score\": evaluator.setMetricName(\"f1\").evaluate(test_preds)\n",
        "}\n",
        "\n",
        "# === Mostrar comparación ===\n",
        "print(\"\\n--- Comparación de métricas ---\")\n",
        "for metric in train_metrics:\n",
        "    print(f\"{metric}:\")\n",
        "    print(f\"  Train: {train_metrics[metric]:.4f}\")\n",
        "    print(f\"  Test : {test_metrics[metric]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwfDtoLUOzmg",
        "outputId": "bd464ba3-b218-4461-c9a6-40955bff25e5"
      },
      "id": "EwfDtoLUOzmg",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparación de métricas ---\n",
            "Accuracy:\n",
            "  Train: 0.6575\n",
            "  Test : 0.6556\n",
            "Precision:\n",
            "  Train: 0.6579\n",
            "  Test : 0.6547\n",
            "Recall:\n",
            "  Train: 0.6575\n",
            "  Test : 0.6556\n",
            "F1-score:\n",
            "  Train: 0.5783\n",
            "  Test : 0.5754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# === Ensamblador de características ===\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"price_scaled\", \"event_day_indexed\", \"event_hour_indexed\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# === Definición del modelo Random Forest con hiperparámetros ajustados ===\n",
        "rf = RandomForestClassifier(\n",
        "    labelCol=\"event_type_indexed\",\n",
        "    featuresCol=\"features\",\n",
        "    numTrees=200,\n",
        "    maxDepth=15,\n",
        "    minInstancesPerNode=5,\n",
        "    maxBins=2000,\n",
        "    impurity=\"gini\",\n",
        "    featureSubsetStrategy=\"sqrt\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# === Pipeline de ensamblado y modelo ===\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# === Entrenamiento del modelo ===\n",
        "model = pipeline.fit(train_scaled)\n",
        "\n",
        "# === Evaluador ===\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"event_type_indexed\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "\n",
        "# === Evaluación en conjunto de entrenamiento ===\n",
        "train_preds = model.transform(train_scaled)\n",
        "train_metrics = {\n",
        "    \"Accuracy\": evaluator.setMetricName(\"accuracy\").evaluate(train_preds),\n",
        "    \"Precision\": evaluator.setMetricName(\"weightedPrecision\").evaluate(train_preds),\n",
        "    \"Recall\": evaluator.setMetricName(\"weightedRecall\").evaluate(train_preds),\n",
        "    \"F1-score\": evaluator.setMetricName(\"f1\").evaluate(train_preds)\n",
        "}\n",
        "\n",
        "# === Evaluación en conjunto de prueba ===\n",
        "test_preds = model.transform(test_scaled)\n",
        "test_metrics = {\n",
        "    \"Accuracy\": evaluator.setMetricName(\"accuracy\").evaluate(test_preds),\n",
        "    \"Precision\": evaluator.setMetricName(\"weightedPrecision\").evaluate(test_preds),\n",
        "    \"Recall\": evaluator.setMetricName(\"weightedRecall\").evaluate(test_preds),\n",
        "    \"F1-score\": evaluator.setMetricName(\"f1\").evaluate(test_preds)\n",
        "}\n",
        "\n",
        "# === Mostrar comparación ===\n",
        "print(\"\\n--- Comparación de métricas ---\")\n",
        "for metric in train_metrics:\n",
        "    print(f\"{metric}:\")\n",
        "    print(f\"  Train: {train_metrics[metric]:.4f}\")\n",
        "    print(f\"  Test : {test_metrics[metric]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I01oy20XOeBm",
        "outputId": "c8a28cc4-5640-481d-e523-32258d403274"
      },
      "id": "I01oy20XOeBm",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparación de métricas ---\n",
            "Accuracy:\n",
            "  Train: 0.6802\n",
            "  Test : 0.6689\n",
            "Precision:\n",
            "  Train: 0.6976\n",
            "  Test : 0.6749\n",
            "Recall:\n",
            "  Train: 0.6802\n",
            "  Test : 0.6689\n",
            "F1-score:\n",
            "  Train: 0.6215\n",
            "  Test : 0.6076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe711e3f-087a-4fa0-932c-2bf70eca2a5d",
      "metadata": {
        "id": "fe711e3f-087a-4fa0-932c-2bf70eca2a5d"
      },
      "source": [
        "## Modelo No Supervisado K-Means"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import BisectingKMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "import pandas as pd\n",
        "\n",
        "# === VectorAssembler (si aún no se ha aplicado) ===\n",
        "clustering_features = [\"price_scaled\", \"event_day_indexed\", \"event_hour_indexed\"]\n",
        "assembler = VectorAssembler(inputCols=clustering_features, outputCol=\"features\")\n",
        "\n",
        "train_vector = assembler.transform(train_scaled).select(\"features\")\n",
        "test_vector = assembler.transform(test_scaled).select(\"features\")\n",
        "\n",
        "# === Evaluador de Silhouette Score ===\n",
        "evaluator = ClusteringEvaluator(\n",
        "    featuresCol=\"features\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"silhouette\"\n",
        ")\n",
        "\n",
        "# === Probar múltiples valores de k ===\n",
        "results = []\n",
        "\n",
        "for k in range(2, 6):  # Puedes ampliar este rango\n",
        "    print(f\"\\nEvaluando BisectingKMeans con k={k}\")\n",
        "\n",
        "    bkm = BisectingKMeans(\n",
        "        k=k,\n",
        "        seed=42,\n",
        "        maxIter=50,\n",
        "        featuresCol=\"features\",\n",
        "        predictionCol=\"prediction\"\n",
        "    )\n",
        "\n",
        "    model = bkm.fit(train_vector)\n",
        "\n",
        "    train_preds = model.transform(train_vector)\n",
        "    test_preds = model.transform(test_vector)\n",
        "\n",
        "    silhouette_train = evaluator.evaluate(train_preds)\n",
        "    silhouette_test = evaluator.evaluate(test_preds)\n",
        "    sse = model.computeCost(train_vector)\n",
        "\n",
        "    results.append({\n",
        "        \"k\": k,\n",
        "        \"Silhouette (Train)\": round(silhouette_train, 4),\n",
        "        \"Silhouette (Test)\": round(silhouette_test, 4),\n",
        "        \"SSE (Train)\": round(sse, 2)\n",
        "    })\n",
        "\n",
        "# === Mostrar tabla ordenada por Silhouette Test Score ===\n",
        "df_results = pd.DataFrame(results).sort_values(by=\"Silhouette (Test)\", ascending=False)\n",
        "\n",
        "from IPython.display import display\n",
        "print(\"\\n--- Comparación de modelos BisectingKMeans ---\")\n",
        "display(df_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "mPzrhFiRocoy",
        "outputId": "920207fd-9375-4a8a-c015-963124ec2ea0"
      },
      "id": "mPzrhFiRocoy",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluando BisectingKMeans con k=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/ml/clustering.py:1016: FutureWarning: Deprecated in 3.0.0. It will be removed in future versions. Use ClusteringEvaluator instead. You can also get the cost on the training dataset in the summary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluando BisectingKMeans con k=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/ml/clustering.py:1016: FutureWarning: Deprecated in 3.0.0. It will be removed in future versions. Use ClusteringEvaluator instead. You can also get the cost on the training dataset in the summary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluando BisectingKMeans con k=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/ml/clustering.py:1016: FutureWarning: Deprecated in 3.0.0. It will be removed in future versions. Use ClusteringEvaluator instead. You can also get the cost on the training dataset in the summary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluando BisectingKMeans con k=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/ml/clustering.py:1016: FutureWarning: Deprecated in 3.0.0. It will be removed in future versions. Use ClusteringEvaluator instead. You can also get the cost on the training dataset in the summary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparación de modelos BisectingKMeans ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   k  Silhouette (Train)  Silhouette (Test)  SSE (Train)\n",
              "0  2              0.6965             0.6972  11352677.25\n",
              "2  4              0.4906             0.4900   6210845.38\n",
              "3  5              0.4770             0.4773   5347390.95\n",
              "1  3              0.4449             0.4445   9221124.20"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9114566d-99d0-41a5-be7f-d75f377eb496\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>Silhouette (Train)</th>\n",
              "      <th>Silhouette (Test)</th>\n",
              "      <th>SSE (Train)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0.6965</td>\n",
              "      <td>0.6972</td>\n",
              "      <td>11352677.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0.4906</td>\n",
              "      <td>0.4900</td>\n",
              "      <td>6210845.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>0.4770</td>\n",
              "      <td>0.4773</td>\n",
              "      <td>5347390.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0.4449</td>\n",
              "      <td>0.4445</td>\n",
              "      <td>9221124.20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9114566d-99d0-41a5-be7f-d75f377eb496')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9114566d-99d0-41a5-be7f-d75f377eb496 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9114566d-99d0-41a5-be7f-d75f377eb496');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9ca172cf-2aff-4d57-89f0-fe19f3d7ebaf\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ca172cf-2aff-4d57-89f0-fe19f3d7ebaf')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9ca172cf-2aff-4d57-89f0-fe19f3d7ebaf button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_c377ecf2-c59f-4727-9c73-0ec202724f64\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c377ecf2-c59f-4727-9c73-0ec202724f64 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2,\n        \"max\": 5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          4,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Silhouette (Train)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11444848914103964,\n        \"min\": 0.4449,\n        \"max\": 0.6965,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.4906,\n          0.4449,\n          0.6965\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Silhouette (Test)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1149102983490456,\n        \"min\": 0.4445,\n        \"max\": 0.6972,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.49,\n          0.4445,\n          0.6972\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SSE (Train)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2766745.3555315146,\n        \"min\": 5347390.95,\n        \"max\": 11352677.25,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          6210845.38,\n          9221124.2,\n          11352677.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612a78a1-3f28-48b0-999a-abf1004781a2",
      "metadata": {
        "id": "612a78a1-3f28-48b0-999a-abf1004781a2"
      },
      "source": [
        "# 5. Análisis de resultados\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección se presenta un análisis detallado de los modelos de aprendizaje supervisado (Random Forest) y no supervisado (Bisecting KMeans), con base en las métricas obtenidas, los hiperparámetros utilizados y las decisiones de preprocesamiento, particularmente la partición de la muestra en subconjuntos \\( M_i \\).\n",
        "\n",
        "---\n",
        "\n",
        "### Modelos Supervisados – Random Forest\n",
        "\n",
        "Se implementaron y compararon tres configuraciones del modelo `RandomForestClassifier`, variando los hiperparámetros `numTrees`, `maxDepth`, `minInstancesPerNode`, `maxBins` y `featureSubsetStrategy`. Los resultados fueron los siguientes:\n",
        "\n",
        "| Modelo          | numTrees | maxDepth | minInstancesPerNode | maxBins | featureSubsetStrategy | Accuracy (Train) | Accuracy (Test) | Precision (Train) | Precision (Test) | Recall (Train) | Recall (Test) | F1-score (Train) | F1-score (Test) |\n",
        "|-----------------|----------|----------|----------------------|---------|------------------------|------------------|-----------------|-------------------|------------------|----------------|---------------|------------------|-----------------|\n",
        "| RF Base         | 100      | 15       | -                    | -       | -                      | 0.6537           | 0.6500          | 0.6408            | 0.6338           | 0.6537         | 0.6500        | 0.5884           | 0.5842          |\n",
        "| RF Mid          | 100      | 10       | -                    | 2000    | -                      | 0.6575           | 0.6556          | 0.6579            | 0.6547           | 0.6575         | 0.6556        | 0.5783           | 0.5754          |\n",
        "| RF Optimizado   | 200      | 15       | 5                    | 2000    | sqrt                   | 0.6802           | 0.6689          | 0.6976            | 0.6749           | 0.6802         | 0.6689        | 0.6215           | 0.6076          |\n",
        "\n",
        "**Fortalezas**  \n",
        "- El modelo Random Forest mostró buena generalización, con métricas muy similares entre entrenamiento y prueba.  \n",
        "- La configuración optimizada logró mejorar significativamente el rendimiento general, en especial en F1-score.  \n",
        "- El uso de `featureSubsetStrategy=\"sqrt\"` y `minInstancesPerNode=5` redujo el sobreajuste y mejoró la estabilidad.\n",
        "\n",
        "**Áreas de oportunidad**  \n",
        "- Las métricas aún no superan el umbral del 70%, lo que indica que el modelo tiene espacio de mejora.  \n",
        "- Se podrían explorar nuevas variables, técnicas de ingeniería de características o más combinaciones de hiperparámetros mediante validación cruzada.\n",
        "\n",
        "---\n",
        "\n",
        "### Modelos No Supervisados – Bisecting KMeans\n",
        "\n",
        "Se evaluó el rendimiento del modelo `BisectingKMeans` con distintos valores de `k` (de 2 a 5), utilizando como criterios de evaluación el `Silhouette Score` y la `SSE (Suma de errores cuadráticos)`:\n",
        "\n",
        "| k | Silhouette (Train) | Silhouette (Test) | SSE (Train)     |\n",
        "|---|---------------------|--------------------|------------------|\n",
        "| 2 | 0.6965              | 0.6972             | 11352677.25      |\n",
        "| 4 | 0.4906              | 0.4900             | 6210845.38       |\n",
        "| 5 | 0.4770              | 0.4773             | 5347390.95       |\n",
        "| 3 | 0.4449              | 0.4445             | 9221124.20       |\n",
        "\n",
        "**Fortalezas**  \n",
        "- El valor óptimo de `k=2` obtuvo el mayor Silhouette Score (~0.6972), lo que indica que los datos están bien agrupados en dos clústeres principales.  \n",
        "- Las diferencias entre las métricas de entrenamiento y prueba fueron mínimas, lo que indica buena generalización del modelo no supervisado.\n",
        "\n",
        "**Áreas de oportunidad**  \n",
        "- Aunque se detectó una buena segmentación interna, el modelo no permite interpretar directamente las etiquetas.  \n",
        "- Se podrían analizar las características promedio de cada clúster para identificar perfiles de comportamiento más claramente.\n",
        "\n",
        "---\n",
        "\n",
        "### Reflexión sobre la división en subconjuntos \\( M_i \\)\n",
        "\n",
        "Como parte fundamental del enfoque Big Data, la muestra representativa \\( M \\) fue dividida en subconjuntos \\( M_i \\) con base en la variable de caracterización `event_day`. Esta división permitió generar conjuntos de entrenamiento y prueba balanceados y representativos, evitando sesgos derivados de la distribución temporal de los eventos.\n",
        "\n",
        "Aunque la división en \\( M_i \\) no tuvo un impacto directo en el incremento de métricas como el F1-score o la accuracy, fue esencial para asegurar:\n",
        "- Que cada partición tuviera una representación equitativa de las clases (`view`, `cart`, `purchase`).\n",
        "- Que los conjuntos derivados conservaran la estructura natural de los datos, favoreciendo la estabilidad del entrenamiento.\n",
        "- Que los modelos no se vieran influenciados por posibles sesgos de días con pocos eventos o distribuciones atípicas.\n",
        "\n",
        "Esta práctica también simula escenarios reales de análisis por bloques o regiones, donde los datos se deben procesar de forma distribuida o segmentada.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusión General\n",
        "\n",
        "El modelo Random Forest permitió predecir de forma razonable los tipos de eventos (`view`, `cart`, `purchase`) y mejoró con ajustes de hiperparámetros. Por otro lado, Bisecting KMeans reveló una estructura clara en los datos al detectar dos agrupaciones naturales consistentes.\n",
        "\n",
        "Ambos enfoques demostraron su utilidad: Random Forest para clasificación predictiva y Bisecting KMeans para análisis exploratorio. El proceso mostró la importancia de dividir correctamente la muestra, ajustar hiperparámetros cuidadosamente y evaluar tanto en conjuntos de entrenamiento como de prueba para validar la calidad de los resultados.\n"
      ],
      "metadata": {
        "id": "gU1f9lhrwmmq"
      },
      "id": "gU1f9lhrwmmq"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}